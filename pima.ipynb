{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pima dataset\n",
    "label_idx = {'0': 0, '1': 1}\n",
    "class PimaDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "           \n",
    "    def __getitem__(self, index):\n",
    "        item = self.data.iloc[index].values    \n",
    "        return (item[0:8].astype(np.float32), item[8].astype(np.int))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "\n",
    "    def get_datasets(pima_file, train_ratio=0.60,valid_ratio=0.20,test_ratio=0.20):\n",
    "\n",
    "        labels = {'class': label_idx}\n",
    "        df = pd.read_csv(pima_file)\n",
    "        data=(df-df.mean())/df.std()\n",
    "        print(data.head())\n",
    "        data.replace(labels, inplace=True)\n",
    "\n",
    "        train_df = data.sample(frac=train_ratio, random_state=10)\n",
    "        validation_df = data.sample(frac=valid_ratio, random_state=10)\n",
    "        test_df = data.sample(frac=test_ratio, random_state=10)\n",
    "\n",
    "        return PimaDataset(train_df), PimaDataset(validation_df),PimaDataset(test_df)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pima_file = \"diabetes.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Pregnancies   Glucose  BloodPressure  SkinThickness   Insulin       BMI  \\\n",
      "0     0.639530  0.847771       0.149543       0.906679 -0.692439  0.203880   \n",
      "1    -0.844335 -1.122665      -0.160441       0.530556 -0.692439 -0.683976   \n",
      "2     1.233077  1.942458      -0.263769      -1.287373 -0.692439 -1.102537   \n",
      "3    -0.844335 -0.997558      -0.160441       0.154433  0.123221 -0.493721   \n",
      "4    -1.141108  0.503727      -1.503707       0.906679  0.765337  1.408828   \n",
      "\n",
      "   DiabetesPedigreeFunction       Age   Outcome  \n",
      "0                  0.468187  1.425067  1.365006  \n",
      "1                 -0.364823 -0.190548 -0.731643  \n",
      "2                  0.604004 -0.105515  1.365006  \n",
      "3                 -0.920163 -1.040871 -0.731643  \n",
      "4                  5.481337 -0.020483  1.365006  \n"
     ]
    }
   ],
   "source": [
    "# Get train, validation and test data\n",
    "train_ds,validation_ds,test_ds = PimaDataset.get_datasets(pima_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# instances in training set:  461\n",
      "# instances in validation set:  154\n",
      "# instances in test set:  154\n"
     ]
    }
   ],
   "source": [
    "print('# instances in training set: ', len(train_ds))\n",
    "print('# instances in validation set: ', len(validation_ds))\n",
    "print('# instances in test set: ', len(test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataloader\n",
    "batch_size = 50\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_ds, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset=validation_ds, batch_size=batch_size, shuffle=True)\n",
    "test_loader  = torch.utils.data.DataLoader(dataset=test_ds, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create net\n",
    "import torch.nn.functional as F\n",
    "class PimaNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden1_size,hidden2_size,num_classes):\n",
    "        \n",
    "        super(PimaNet, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_size, hidden1_size)\n",
    "        #self.bn_layer = nn.BatchNorm1d(hidden1_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden1_size, hidden2_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden2_size, num_classes)   \n",
    "       \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.log_softmax(self.fc3(x), dim=1)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PimaNet(\n",
      "  (fc1): Linear(in_features=8, out_features=200, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (fc2): Linear(in_features=200, out_features=100, bias=True)\n",
      "  (relu2): ReLU()\n",
      "  (fc3): Linear(in_features=100, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = PimaNet(8, 200,100, 2)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set loss,learning rate and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.001\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/50,Training loss:0.633804440498352,Validation loss: 0.7204402387142181,validation Accuracy: 62.25%\n",
      "Epoch: 2/50,Training loss:0.5712051048874855,Validation loss: 0.5960188806056976,validation Accuracy: 78.25%\n",
      "Epoch: 3/50,Training loss:0.5382442851861319,Validation loss: 0.5320881567895412,validation Accuracy: 83.0%\n",
      "Epoch: 4/50,Training loss:0.5209242649376393,Validation loss: 0.5038420176133513,validation Accuracy: 84.0%\n",
      "Epoch: 5/50,Training loss:0.5039063984155655,Validation loss: 0.5052298314869403,validation Accuracy: 79.25%\n",
      "Epoch: 6/50,Training loss:0.495514190196991,Validation loss: 0.4919347061465184,validation Accuracy: 80.25%\n",
      "Epoch: 7/50,Training loss:0.4887523634093149,Validation loss: 0.4857757054269314,validation Accuracy: 74.5%\n",
      "Epoch: 8/50,Training loss:0.48114571832120423,Validation loss: 0.5003880797885358,validation Accuracy: 73.99999237060547%\n",
      "Epoch: 9/50,Training loss:0.4743063373698129,Validation loss: 0.5045739457839065,validation Accuracy: 74.0%\n",
      "Epoch: 10/50,Training loss:0.47099016845226294,Validation loss: 0.5011294689029455,validation Accuracy: 80.25%\n",
      "Epoch: 11/50,Training loss:0.4668504666198384,Validation loss: 0.4873104759237983,validation Accuracy: 86.0%\n",
      "Epoch: 12/50,Training loss:0.46320054059227317,Validation loss: 0.488203061123689,validation Accuracy: 79.25%\n",
      "Epoch: 13/50,Training loss:0.45869057075335434,Validation loss: 0.4883941990824846,validation Accuracy: 74.5%\n",
      "Epoch: 14/50,Training loss:0.45357148157698773,Validation loss: 0.47839852342648165,validation Accuracy: 84.0%\n",
      "Epoch: 15/50,Training loss:0.45013586799303695,Validation loss: 0.4705767594277859,validation Accuracy: 85.0%\n",
      "Epoch: 16/50,Training loss:0.446280886605382,Validation loss: 0.4693639015313238,validation Accuracy: 82.75%\n",
      "Epoch: 17/50,Training loss:0.4424914542366477,Validation loss: 0.4758051105720155,validation Accuracy: 82.25%\n",
      "Epoch: 18/50,Training loss:0.4389446105394099,Validation loss: 0.46732872465832365,validation Accuracy: 85.0%\n",
      "Epoch: 19/50,Training loss:0.4368360976639548,Validation loss: 0.46425441211383595,validation Accuracy: 76.0%\n",
      "Epoch: 20/50,Training loss:0.43379580259323125,Validation loss: 0.4578639707528055,validation Accuracy: 85.0%\n",
      "Epoch: 21/50,Training loss:0.430184434638137,Validation loss: 0.4510090598570449,validation Accuracy: 85.5%\n",
      "Epoch: 22/50,Training loss:0.42669721381230796,Validation loss: 0.44996565571901476,validation Accuracy: 75.5%\n",
      "Epoch: 23/50,Training loss:0.42352113114750906,Validation loss: 0.4436182056594154,validation Accuracy: 87.5%\n",
      "Epoch: 24/50,Training loss:0.4204738986988863,Validation loss: 0.4393628780574848,validation Accuracy: 85.49999237060547%\n",
      "Epoch: 25/50,Training loss:0.41817985433340077,Validation loss: 0.4349523151665926,validation Accuracy: 82.25%\n",
      "Epoch: 26/50,Training loss:0.4155612974785842,Validation loss: 0.430565693988823,validation Accuracy: 86.49999237060547%\n",
      "Epoch: 27/50,Training loss:0.4127930989419973,Validation loss: 0.42592491892476875,validation Accuracy: 88.5%\n",
      "Epoch: 28/50,Training loss:0.40985249903585236,Validation loss: 0.42315796623006463,validation Accuracy: 88.0%\n",
      "Epoch: 29/50,Training loss:0.4079171336416541,Validation loss: 0.41936076862801763,validation Accuracy: 87.5%\n",
      "Epoch: 30/50,Training loss:0.40699849799275406,Validation loss: 0.421629721360902,validation Accuracy: 76.5%\n",
      "Epoch: 31/50,Training loss:0.404847902780579,Validation loss: 0.41871167785458024,validation Accuracy: 82.25%\n",
      "Epoch: 32/50,Training loss:0.4024743277113885,Validation loss: 0.4159847108530812,validation Accuracy: 83.75%\n",
      "Epoch: 33/50,Training loss:0.40065485487381625,Validation loss: 0.4114161008178736,validation Accuracy: 86.5%\n",
      "Epoch: 34/50,Training loss:0.398958178346648,Validation loss: 0.4076561192324495,validation Accuracy: 90.0%\n",
      "Epoch: 35/50,Training loss:0.3970222605551993,Validation loss: 0.4046945527994207,validation Accuracy: 88.5%\n",
      "Epoch: 36/50,Training loss:0.3944136977609662,Validation loss: 0.4007932245472653,validation Accuracy: 91.0%\n",
      "Epoch: 37/50,Training loss:0.3919626717229148,Validation loss: 0.39756906593872887,validation Accuracy: 84.25%\n",
      "Epoch: 38/50,Training loss:0.38942110314965256,Validation loss: 0.39426991782200177,validation Accuracy: 90.5%\n",
      "Epoch: 39/50,Training loss:0.3870791355959881,Validation loss: 0.39061795602528715,validation Accuracy: 90.0%\n",
      "Epoch: 40/50,Training loss:0.38477675901725894,Validation loss: 0.3888741851551458,validation Accuracy: 89.5%\n",
      "Epoch: 41/50,Training loss:0.38288215037162715,Validation loss: 0.389905737917416,validation Accuracy: 73.24999237060547%\n",
      "Epoch: 42/50,Training loss:0.3809064694990715,Validation loss: 0.38686917817574884,validation Accuracy: 90.0%\n",
      "Epoch: 43/50,Training loss:0.37880398601640103,Validation loss: 0.3836649000254828,validation Accuracy: 91.5%\n",
      "Epoch: 44/50,Training loss:0.37701481812718235,Validation loss: 0.38421382270330057,validation Accuracy: 83.75%\n",
      "Epoch: 45/50,Training loss:0.37463937482900084,Validation loss: 0.38154293437384895,validation Accuracy: 90.5%\n",
      "Epoch: 46/50,Training loss:0.37341188087087607,Validation loss: 0.37813635253468936,validation Accuracy: 93.0%\n",
      "Epoch: 47/50,Training loss:0.37196757151408394,Validation loss: 0.3765058151466098,validation Accuracy: 82.75%\n",
      "Epoch: 48/50,Training loss:0.37026319784733147,Validation loss: 0.3769549049320631,validation Accuracy: 85.25%\n",
      "Epoch: 49/50,Training loss:0.3678622915884669,Validation loss: 0.3739319705043216,validation Accuracy: 91.5%\n",
      "Epoch: 50/50,Training loss:0.36568755836784833,Validation loss: 0.3713566376827657,validation Accuracy: 85.74999237060547%\n"
     ]
    }
   ],
   "source": [
    "# train the data and check for validation\n",
    "num_epochs = 50\n",
    "\n",
    "train_losses, valid_losses = [], []\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0\n",
    "    for i, (items, classes) in enumerate(train_loader):\n",
    "    \n",
    "        net.train()           \n",
    "\n",
    "        optimizer.zero_grad() \n",
    "        outputs = net(items)  \n",
    "        loss = criterion(outputs, classes) \n",
    "        loss.backward()       \n",
    "        optimizer.step()      \n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    else:\n",
    "        net.eval()\n",
    "        valid_loss = 0\n",
    "        accuracy = 0\n",
    "        with torch.no_grad():\n",
    "            for i, (items, classes) in enumerate(validation_loader):\n",
    "                outputs = net(items)\n",
    "                loss = criterion(outputs,classes)\n",
    "                valid_loss += loss.item()\n",
    "                ps = torch.exp(outputs)\n",
    "                \n",
    "                top_p, top_class = ps.topk(1, dim=1)\n",
    "                labels = classes.view(*top_class.shape)\n",
    "                \n",
    "                equals = top_class == labels\n",
    "                accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "                \n",
    "                \n",
    "        train_losses.append(running_loss/len(train_loader))\n",
    "        valid_losses.append(valid_loss/len(validation_loader))\n",
    "        print(f'Epoch: {epoch+1}/{num_epochs},Training loss:{sum(train_losses)/len(train_losses)},Validation loss: {sum(valid_losses)/len(valid_losses)},validation Accuracy: {accuracy/len(validation_loader)*100}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x123c2d128>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plot training and validation losses\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(valid_losses, label='Validation loss')\n",
    "plt.legend(frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 91.5%\n",
      "Predicted Outcome:tensor([1, 1, 0, 0])\n",
      "Actual Outcome:tensor([1, 1, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "# predict test data:\n",
    "accuracy = 0\n",
    "with torch.no_grad():    \n",
    "    for i, (items, classes) in enumerate(test_loader):\n",
    "        outputs = net(items)\n",
    "        \n",
    "        ps = torch.exp(outputs)\n",
    "\n",
    "        top_p, top_class = ps.topk(1, dim=1)\n",
    "        labels = classes.view(*top_class.shape)\n",
    "\n",
    "        equals = top_class == labels\n",
    "        accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "        \n",
    "print(f'Accuracy: {accuracy/len(test_loader)*100}%')\n",
    "print(f'Predicted Outcome:{top_class.view(*classes.shape)}')\n",
    "print(f'Actual Outcome:{classes}')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
